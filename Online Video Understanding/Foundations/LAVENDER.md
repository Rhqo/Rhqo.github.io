통합 비전-언어 프레임워크는 최근 몇 년 동안 크게 발전했으며, 대부분은 이미지-텍스트 작업을 시퀀스-투-시퀀스 생성으로 통합하기 위해 인코더-디코더 아키텍처를 채택하고 있습니다. 그러나 기존의 비디오-언어(VidL) 모델은 여전히 각 작업에 대해 모델 아키텍처와 훈련 목표가 특정 작업에 맞추어져 있어야 합니다. 본 연구에서는 통합 VidL 프레임워크인 LAVENDER를 탐구합니다. LAVENDER는 Masked Language Modeling (MLM)을 모든 사전 훈련 및 다운스트림 작업에 대한 공통 인터페이스로 사용합니다. 이러한 통합은 단순화된 모델 아키텍처를 가져오며, 다중 모달 인코더 위에 무거운 디코더 대신 경량의 MLM 헤드만 필요합니다.

놀랍게도, 실험 결과는 이 통합 프레임워크가 비디오 질문 응답, 텍스트-비디오 검색 및 비디오 캡셔닝을 포함한 14개의 VidL 벤치마크에서 경쟁력 있는 성능을 달성함을 보여줍니다. 추가 분석을 통해 LAVENDER가 기존 VidL 방법에 비해 다음과 같은 장점을 가지고 있음을 보여줍니다: (i) 다중 작업 미세 조정 시 단일 파라미터 세트로 모든 다운스트림 작업을 지원; (ii) 다양한 다운스트림 작업에 대한 소수 샷 일반화; (iii) 비디오 질문 응답 작업에 대한 제로샷 평가 가능.