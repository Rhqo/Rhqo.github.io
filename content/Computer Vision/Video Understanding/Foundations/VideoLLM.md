비디오 데이터의 기하급수적인 증가로 인해 비디오 콘텐츠를 자동으로 분석하고 이해할 수 있는 기술에 대한 필요성이 커지고 있습니다. 그러나 기존의 비디오 이해 모델은 종종 특정 작업에만 특화되어 있으며 다양한 작업을 처리할 수 있는 포괄적인 능력이 부족합니다. GPT와 같은 대형 언어 모델(LLM)의 성공은 시퀀스 인과 추론에서 인상적인 능력을 보여주었습니다. 이 통찰을 바탕으로, 우리는 자연어 처리(NLP)에서 사전 훈련된 LLM의 시퀀스 추론 능력을 활용한 새로운 프레임워크인 VideoLLM을 제안합니다.

VideoLLM은 모달리티 인코더와 의미 변환기(Semantic Translator)를 신중하게 설계하여 다양한 모달리티의 입력을 통합된 토큰 시퀀스로 변환합니다. 이 토큰 시퀀스는 디코더 전용 LLM에 입력됩니다. 이후 간단한 작업 헤드를 통해, VideoLLM은 다양한 비디오 이해 작업을 위한 효과적인 통합 프레임워크를 제공합니다.

VideoLLM의 효능을 평가하기 위해, 우리는 여러 LLM과 미세 조정 방법을 사용하여 광범위한 실험을 수행하였습니다. 네 개의 데이터셋에서 소싱된 여덟 가지 작업에 대해 VideoLLM을 평가한 결과, LLM의 이해 및 추론 능력이 비디오 이해 작업에 효과적으로 전이될 수 있음을 보여주었습니다.