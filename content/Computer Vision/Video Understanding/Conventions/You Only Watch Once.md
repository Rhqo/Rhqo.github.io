# You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization

![[YOWO_0.png]]

YOWO architecture는 크게 4가지 부분으로 나눌 수 있다.
- 3D-CNN Branch
- 2D-CNN Branch
- CFAM
- bounding box regression

## 3D-CNN Branch
인간 행동 이해를 위해 맥락 정보가 중요하므로, 3D-CNN을 사용해 시공간적 특징을 추출합니다. 3D-CNN은 공간 차원뿐만 아니라 시간 차원에서도 convolution 연산을 수행하여 움직임 정보를 캡처할 수 있습니다. 이 프레임워크의 기본 3D-CNN 아키텍처는 Kinetics 데이터셋에서 성능이 뛰어난 3D-ResNext-101을 사용합니다. 3D 네트워크의 입력은 시간 순서로 연속적인 프레임 시퀀스로 구성된 비디오 클립이며, 출력은 2D-CNN의 출력과 맞추기 위해 깊이 차원이 1로 줄어듭니다.

C는 채널 수 (보통 3), D는 입력 프레임 수(깊이)
입력 : \[$C \times D \times H \times W$]

D'의 크기가 1로 줄어든다.
출력 : \[$C' \times D' \times H' \times W'$] = \[$C' \times H' \times W'$]          ($H' = \frac{H}{32}$, $W' = \frac{W}{32}$)

## 2D-CNN Branch
동시에 공간적 위치 문제를 해결하기 위해, 키 프레임의 2D 특징을 병렬로 추출합니다. 2D CNN 분기의 기본 아키텍처로는 정확도와 효율성 간의 균형이 좋은 Darknet-19를 사용합니다. 입력 클립의 가장 최근 프레임인 키 프레임의 출력 특징 맵은 3D-CNN의 경우와 유사한 모양을 가집니다.

C는 채널 수 (보통 3)
입력 : \[$C \times H \times W$]


출력 : \[$C'' \times H' \times W'$]     ($H' = \frac{H}{32}$, $W' = \frac{W}{32}$)

## Channel Fusion and Attention Mechanism (CFAM)
채널을 따라 특징을 단순히 쌓는 방식인 연결을 사용하여 두 특징 맵을 융합합니다. 그 결과, 융합된 특징 맵은 움직임 정보와 외형 정보를 모두 인코딩하게 되며, 이를 CFAM 모듈의 입력으로 사용합니다.

CFAM 모듈은 Gram 행렬을 기반으로 채널 간 종속성을 매핑합니다. Gram 행렬 기반의 attention 메커니즘은 다양한 출처에서 나온 특징을 합리적으로 융합하는 데 유익하며, 전체 성능을 크게 향상시킵니다.

연결된 특징 맵 $A \in R^{(C'+C'')×H×W}$는 2D 및 3D 정보의 갑작스러운 조합으로 간주될 수 있으며, 이들 간의 상호 관계를 무시합니다. 따라서, 먼저 A를 두 개의 convolution 계층에 통과시켜 새로운 특징 맵 $B \in R^{C×H'×W'}$를 생성합니다. 이후, 특징 맵 B에 몇 가지 작업을 수행합니다.
$$B \in R^{C×H×W} → F \in R^{C×N},(N = H \times W)$$

$F$와 $F^T$를 사용하여 Gram 행렬 수행
$$G=F⋅F^T,G_{ij}​ = \sum_{k=1}^N​F_{ik}​ ⋅ F_{jk}​$$
Softmax layer를 적용하여 channel attention map $M$ 생성
$$M_{ij}​=\frac{exp(G_{ij}​)}{\sum_{j=1}^C​exp(G_{ij}​)}​$$
M과 F 사이의 추가적인 행렬곱 수행, 결과를 3차원으로 재구성
$$F' = M⋅F → F'' \in R^{C×H×W}$$
channel attention 모듈의 출력은 원래 입력 특징 맵 $B$와 학습 가능한 스칼라 파라미터 $\alpha$를 사용한 element-wise 합연산을 통해 결합 ($C \in R^{C×H'×W'}$)
$$C = \alpha⋅F'' + B$$
특징 맵 C는 2개의 추가 convolution layer로 입력되어 CFAM 모듈의 출력 특징 맵 D 를 생성
$$D \in R^{C^∗ × H' × W'}$$
이러한 아키텍처는 채널 간 상호 종속성을 고려한 특징의 표현력을 높이며, 다른 분기로부터 나온 특징들이 합리적이고 부드럽게 집계될 수 있도록 합니다. Gram 행렬은 전체 특징 맵을 고려하여, 평탄화된 특징 벡터 쌍의 내적이 그들 사이의 관계를 나타내며, 특정 채널의 경우, 더 큰 영향을 미치는 다른 채널에 더 많은 가중치를 할당합니다. 이 메커니즘을 통해 맥락적 관계가 강조되고, 특징의 변별력이 향상됩니다.

## Bounding box regression
각 그리드 셀 $H'×W'$에 대해, 해당 데이터셋에서 k-평균(k-means) 기법을 사용하여 5개의 사전 앵커가 선택되며, 클래스별 행동 점수, 4개의 좌표, 그리고 신뢰도 점수를 포함하여 최종 출력 크기는 \[$(5×(NumCls+5))×H'×W'$]가 됩니다. 경계 상자의 회귀는 이러한 앵커를 기준으로 정제됩니다.

훈련과 테스트 시간 모두 입력 해상도는 224 × 224를 사용했습니다. 다양한 해상도로 다중 스케일 훈련을 적용해도 성능 향상이 관찰되지 않았습니다. 손실 함수는 원래의 YOLOv2 네트워크와 유사하게 정의되었으나, 위치 추정을 위해 에서처럼 beta=1로 설정된 Smooth L1 손실을 적용합니다:

$$L1,smooth(x,y)
=\begin{cases} 
	0.5(x-y^2)\quad if|x-y| < 1 \\ |x-y| - 0.5 \quad otherwise
\end{cases}$$

여기서 x와 y는 각각 네트워크 예측값과 실제값을 나타냅니다. Smooth L1 손실은 MSE 손실보다 이상치에 덜 민감하며, 경우에 따라 기울기 폭발을 방지합니다. 신뢰도 점수에 대해서는 여전히 MSE 손실을 적용하며, 이는 다음과 같이 정의됩니다:

$$L_{conf} = L_{MSE}(x,y)=(x−y)^2$$

최종 탐지 손실은 개별 좌표 손실 x,y,너비x, y, \text{너비}x,y,너비 및 높이\text{높이}높이와 신뢰도 점수 손실의 합으로 나타납니다:

$$L_D=L_x+L_y+L_w+L_h+L_{conf}$$
classification을 위해 focal loss를 적용:
$$L_{focal}​(x,y) = y(1−x)^{\gamma}log(x) + (1−y)x^{\gamma}log(1−x)$$
여기서 x는 소프트맥스 처리된 네트워크 예측값이고, $y \in {0,1}$은 실제 클래스 레이블입니다. $\gamma$는 조절 인자로, 높은 신뢰도를 가진 샘플(즉, 쉬운 샘플)의 손실을 줄이고 낮은 신뢰도를 가진 샘플(즉, 어려운 샘플)의 손실을 증가시킵니다. 
그러나 AVA 데이터셋은 다중 레이블 데이터셋으로, 각 사람은 하나의 자세 동작(예: 걷기, 서기 등)과 여러 인간-인간 또는 인간-객체 상호작용 행동을 수행합니다. 따라서 우리는 자세 클래스에는 소프트맥스를, 상호작용 행동에는 시그모이드를 적용했습니다. 또한, AVA는 불균형한 데이터셋이기 때문에 조절 인자 $\gamma$만으로는 데이터셋 불균형 문제를 해결하기에 충분하지 않습니다. 따라서 우리는 포컬 손실의 $\alpha$-균형 변형 을 사용했습니다. $\alpha$ 항을 위해, 클래스 샘플 비율의 지수를 사용했습니다.

YOWO 아키텍처의 최적화를 위해 사용되는 최종 손실은 탐지 손실과 분류 손실의 합으로 정의되며, 다음과 같습니다:
$$L_{final}​=λL_D​+L_{Cls}​$$$\lambda$=0.5가 실험에서 가장 좋은 성능을 보였습니다.